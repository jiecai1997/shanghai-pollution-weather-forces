{
    "collab_server" : "",
    "contents" : "---\ntitle: \"hw6\"\nauthor: \"Jie Cai\"\ndate: \"11/16/2017\"\noutput: html_document\n---\n\n```{r import libraries}\nlibrary(arm)\nlibrary(pROC)\n```\n\n#### Back With the Sesame Street Data\n\n##### 1. Create a new variable that equals zero for all individuals who never watch Sesame Street and equals one for all other individuals. Use the variable viewcat to create this variable. The new binary variable will b e the outcome for the rest of the analysis. Report the percentage of individuals who never watch Sesame Street. \n\n```{r q1}\n#read in datafile\nsesame_df = read.csv('sesame.csv')\nattach(sesame_df)\n\n#make binary variables for viewcat\nn = nrow(sesame_df)\n\nwatched = rep(0, n)\nwatched[viewcat != 1] = 1\n\nnot_watched = rep(0, n)\nnot_watched[viewcat == 1] = 1\n\n#% of individuals who never watch Sesame Street\npercentage_not_watched = sum(not_watched==1)/n*100\npercentage_not_watched\n```\n22.5% of individuals have never watched Sesame Street.\n\n##### 2. Use a logistic regression to examine whether or not encouragement to watch Sesame Street results in higher odds of watching Sesame Street at least once. Do not control for any post-test variables. You need not control for all pre-test variables, although you are welcome to. You should control for site, setting, age, and sex for sure. You decide how you want to account for the predictor variables, e.g., as continuous or categorical predictors, with or without transformations, with or without interactions, etc. In the answer, you should do the following. \n``` {r q2}\n##############################################################################################################\n#Exploratory Data Analysis\n#############################################################################################################\n\n#exploratory analysis with encouragement vs watch\ntapply(watched, viewenc, mean)\n#obvious difference between watch percentage of viewenc and non-viewenc groups\n#% of children who watch sesame street in the non-viewenc group: 54.55%\n#% of children who watch sesame street in the viewenc group: 90.79%\n\n#test for multicollinearity to get rid of unneccessary variables\n#threshold: 0.8\ncontVars = c(\"site\", \"age\")\nsesame_contVars = sesame_df[contVars]\ncor(sesame_contVars)\n#no high values of multicollinearity among any of the variables, no need to remove any\n\n#exploratory analysis with other minimal variables (site, setting, age, sex), unchanged\nbinnedplot(site, watched, xlab = \"site\", ylab = \"watched\") \ntapply(watched, setting, mean)\nbinnedplot(age, watched, xlab = \"age\", ylab = \"watched\") \ntapply(watched, site, mean)\n#no obvious trends, no need to transform\n\n#############################################################################################################\n#The Most Basic Logistic Regression\n#############################################################################################################\n\n#glm_basic: basic logistic regression with basic variables, no changes\nglm_basic = glm(watched ~ viewenc + site + setting + age + sex, family = binomial)\nsummary(glm_basic)\n\n#roc curve\nroc(watched, fitted(glm_basic), plot=T, legacy.axes=T)\n\n#binned residuals\nresid_glm_basic = watched -fitted(glm_basic)\n\ntapply(resid_glm_basic, viewenc, mean)\nbinnedplot(site, resid_glm_basic, xlab = \"site\", ylab = \"watched residuals\") \ntapply(resid_glm_basic, setting, mean)\nbinnedplot(age, resid_glm_basic, xlab = \"age\", ylab = \"watched\") \ntapply(resid_glm_basic, sex, mean)\n\n#confusion matrix\nthreshold = 0.5\ntable(watched, resid_glm_basic > threshold)\n\nthreshold = 0.2\ntable(watched, resid_glm_basic > threshold)\n#no false positives even with a really low threshold, we have a very conservative model\n\n##############################################################################################################\n#Continuous Variables as Categorical Variables\n##############################################################################################################\n\n#exploratory analysis with continuous variables as categorical variables (site, age) so see if we can do so\ntapply(watched, site, mean)\n#seems to be a large difference in watch percentage between those at sites 1-3 and sites 4-5.\n#lets make them into categorical variables with 2 levels\n\nsites_123 = rep(0, n)\nsites_123[site == 1 | site == 2 | site == 3] = 1\n\nsites_45 = rep(0, n)\nsites_45[site == 4 | site == 5] = 1\n\ntapply(watched, age, mean)\n#doesn't seem to be an obvious trend for the ages, hard to turn into a categorical variable\n\n#logistic regression with site as a categorical variable\nglm_categorial = glm(watched ~ viewenc + sites_123 + setting + age + sex, family = binomial)\nsummary(glm_categorial)\n\n#roc curve\nroc(watched, fitted(glm_categorial), plot=T, legacy.axes=T)\n\n#binned residuals\nresid_glm_categorial = watched -fitted(glm_categorial)\n\ntapply(resid_glm_categorial, viewenc, mean)\ntapply(resid_glm_categorial, sites_123, mean)\ntapply(resid_glm_categorial, setting, mean)\nbinnedplot(age, resid_glm_categorial, xlab = \"age\", ylab = \"watched residuals\") \ntapply(resid_glm_categorial, sex, mean)\n#binned residuals look fine, close to 0 and randomly scattered\n\n#confusion matrix\nthreshold = 0.5\ntable(watched, resid_glm_categorial > threshold)\n\nthreshold = 0.2\ntable(watched, resid_glm_categorial > threshold)\n#no false positives even with a really low threshold, we have a very conservative model\n\n##########################################################################################################\n#Test for Interaction Variables\n##########################################################################################################\n\n#test for interaction variables between viewenc and other variables (site, setting, age, sex)\n\n#encouragement & site\ntapply(viewenc, sites_45, mean) \n#doesn't seem to be a large difference/trend between encouragement levels with different sites\n#no need for interaction variable\n\n#encouragement & setting\ntapply(viewenc, setting, mean)\n#pretty big difference/trend between encouragement levels with different settings\n#make interaction variable \nviewenc_setting = viewenc*setting\n\n#encouragement & age\nbinnedplot(age, viewenc, xlab = \"age\", ylab = \"watched\") \n#doesn't seem to be a large difference/trend between encouragement levels with ages\n#no need for interaction variable\n\n#encouragement & sex\ntapply(viewenc, sex, mean) \n#doesn't seem to be a large difference/trend between encouragement levels with sexes\n#no need for interaction variable\n\n#logistic regression with interaction variables\nglm_interact = glm(watched ~ viewenc + site + setting + age + sex + viewenc*setting, family = binomial)\nsummary(glm_interact)\n\n#run ANOVA test between basic GLM and GLM with interactions\nanova(glm_basic, glm_interact, test= \"Chisq\")\n#p = 0.5689 - large p-value indicates that interaction variable is unnceccessary\n#don't use this logistic regression model\n\n#############################################################################################################\n#Forward Selection With Pre-Test Variables\n#############################################################################################################\n\n#test for multicollinearity between pre-test variables before we try forward selection with all of them\n#threshold: 0.8\npretestVars = c(\"prebody\", \"prelet\", \"preform\", \"prenumb\", \"prerelat\", \"preclasf\")\nsesame_pretestVars = sesame_df[pretestVars]\ncor(sesame_pretestVars)\n#no multicollinearity between any of the pre-test variables! will consider all of them individually.\n\n#logistic regression with prebody\nglm_prebody = glm(watched ~ viewenc + sites_123 + setting + age + sex + prebody, family = binomial)\nanova(glm_prebody, glm_basic, test= \"Chisq\")\n#p<0.05, significant\nbinnedplot(prebody, watched, xlab = \"prebody\", ylab = \"watched\") \n#no trend, no need to transform\n\n#logistic regression with prelet\nglm_prelet = glm(watched ~ viewenc + sites_123 + setting + age + sex + prelet, family = binomial)\nanova(glm_prelet, glm_basic, test= \"Chisq\")\n#p>0.05, not significant\n\n#logistic regression with preform\nglm_preform = glm(watched ~ viewenc + sites_123 + setting + age + sex + preform, family = binomial)\nanova(glm_preform, glm_basic, test= \"Chisq\")\n#p<0.05, significant\nbinnedplot(preform, watched, xlab = \"preform\", ylab = \"watched\") \n#logistic trend, try with squared term\nlog_preform = log(preform)\nbinnedplot(log_preform, watched, xlab = \"preform\", ylab = \"watched\") \n#looks better after log tranformation\n\n#logistic regression with prenumb\nglm_prenumb = glm(watched ~ viewenc + sites_123 + setting + age + sex + prenumb, family = binomial)\nanova(glm_prenumb, glm_basic, test= \"Chisq\")\n#p<0.05, significant\nbinnedplot(prenumb, watched, xlab = \"prenumb\", ylab = \"watched\") \n#no curvature trend, no need to transform\n\n#logistic regression with prerelat\nglm_prerelat = glm(watched ~ viewenc + sites_123 + setting + age + sex + prerelat, family = binomial)\nsummary(glm_prerelat)\nanova(glm_prerelat, glm_basic, test= \"Chisq\")\n#p<0.05, significant\nbinnedplot(prerelat, watched, xlab = \"prerelat\", ylab = \"watched\") \n#no curvature trend, no need to transform\n\n#logistic regression with preclasf\nglm_preclasf = glm(watched ~ viewenc + sites_123 + setting + age + sex + preclasf, family = binomial)\nanova(glm_preclasf, glm_basic, test= \"Chisq\")\n#p<0.05, significant\nbinnedplot(preclasf, watched, xlab = \"preform\", ylab = \"watched\") \n#no curvature trend, no need to transform\n\n#logistic regression with all the significant pre-test variables\nglm_pretests = glm(watched ~ viewenc + sites_123 + setting + age + sex + prebody + preform + prenumb + prerelat + preclasf, family = binomial)\nsummary(glm_pretests)\nanova(glm_pretests, glm_basic, test= \"Chisq\")\n#anova p-value: <<<0.05, so these pre-test variables are significant and should be included in our final model.\n\n#############################################################################################################\n#Final Adjustments\n#############################################################################################################\n\n#mean-center continuous variables for easier interpretation\nage_c = age - mean(age)\nprebody_c = prebody - mean(prebody)\nlog_preform_c = log_preform - mean(log_preform)\nprenumb_c = prenumb - mean(prenumb)\nprerelat_c = prerelat - mean(prerelat)\npreclasf_c = preclasf - mean(preclasf)\n\n#use as.factor() for categorical variables in final regression model so we can see what we used as our baseline\n#all of our continuous variables should also be mean-centered\nglm_final = glm(watched ~ as.factor(viewenc) + as.factor(sites_123) + as.factor(setting) + age_c + as.factor(sex) + prebody_c + log_preform_c + prenumb_c + prerelat_c + preclasf_c, family = binomial)\n\n#check model fit\n\n#roc curve\nroc(watched, fitted(glm_final), plot=T, legacy.axes=T)\n\n#binned residuals\nresid_glm_final = watched -fitted(glm_final)\n\ntapply(resid_glm_final, viewenc, mean)\ntapply(resid_glm_final, sites_123, mean)\ntapply(resid_glm_final, setting, mean)\nbinnedplot(age_c, resid_glm_final, xlab = \"mean-centered age\", ylab = \"watched residuals\") \ntapply(resid_glm_final, sex, mean)\nbinnedplot(prebody_c, resid_glm_final, xlab = \"mean-centered pretest on knowledge of body parts\", ylab = \"watched residuals\") \nbinnedplot(log_preform_c, resid_glm_final, xlab = \"mean-centered pretest on forms\", ylab = \"watched residuals\") \nbinnedplot(prenumb_c, resid_glm_final, xlab = \"mean-centered pretest on numbers\", ylab = \"watched residuals\") \nbinnedplot(prerelat_c, resid_glm_final, xlab = \"mean-centered pretest on relational terms\", ylab = \"watched residuals\") \nbinnedplot(preclasf_c, resid_glm_final, xlab = \"mean-centered pretest on classification skills\", ylab = \"watched residuals\") \n#binned residuals look fine, close to 0 and randomly scattered. log_preform_c seems a little non-random but due to the small sample size it's hard to make a very good judgement.\n\n#confusion matrix\nthreshold = 0.5\ntable(watched, resid_glm_final > threshold)\n\nthreshold = 0.2\ntable(watched, resid_glm_final > threshold)\n#no false positives even with a really low threshold, we have a very conservative model\n\n#draw conclusions of our logistic regression\nsummary(glm_final)\nexp(confint.default(glm_final))\n```\n\n#### Conclusions\n\nWe are 95% confident that the average odds that a non-encouraged kid (female, from a more privilaged site, watching at school, with average age and average pre-test scores*) watching Sesame Street regularly is between 2.78460769 and 21.4086133. \n\nWe are 95% that holding all other factors constant, if we change a kid's encourgement from non-encouraged to encouraged, the odds of that child watching Sesame Street regularly will change by a factor between 0.02527236 and 0.1946984.\n\nIn the final logistic regression that I chose, my categorical variables (which are all binary) include viewnc, site, setting and sex. My continous variables are age, prebody, prenumb, preform, prerelat and preclasf, and I mean-centered all of these continous variables. I chose this model because it contains the most amount of control variables, all of which have been tested individually to see if they fit the regression. This regression also produced the largest area (0.8936) under the ROC curve and the lowest AIC score (173.85).\n\nI started with the most basic logistic regression model (watched against viewnc, setting, site, age, sex). I then did a series of procedures to achieve my final model.\n\nThe first checked if there were any continous variables that I could express as categorical variables. Since there was a large disparity of watch frequency between 'low' and 'high' sites, I turned sites into a categoical variable (sites_123, sites_45).\n\nI then checked for interaction variables between viewnc and other variables. The only interaction effect I found was between viewnc and setting. However, after fitting that into a logistic regression and doing an ANOVA test between the interaction regression and the basic regression, there are no interaction variables needed afterall.\n\nI then did forward selection for all the pre-test variables, first by comparing ANOVA tests of regressions with and without the individual pre-test variables. After finding variables that suggested significance, I tested for transformations and found the only tranformation I needed was to log transform preform. I then tested a logistic regression with all of the above relevent pre-test variables, and the ANOVA test between that and the basic logistic regression model shows that these variables should be included.\n\nFor a final touchup, I mean centered all my continous variables so they are easier to interpret, and plotted my final regression model using as.factor() on my categorical variables so I could see what I was using as my baseline. I checked the legitimacy of the model by checking the ROC curve, the individual residual tables/binned plots and the confusion matrix. They clarified that I had a model that I was happy with!\n\n*The pre-test scores we included in our final model: mean-centered pretest on knowledge of body parts, forms, numbers, relational terms and classification skills.",
    "created" : 1513184218944.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1239416696",
    "id" : "EAC71024",
    "lastKnownWriteTime" : 1510818735,
    "last_content_update" : 1510818735,
    "path" : "~/Documents/DUKE/Sophomore/Semester1/STA210/hw/hw5/hw6/hw6.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}