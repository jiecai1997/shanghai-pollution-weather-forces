{
    "collab_server" : "",
    "contents" : "---\ntitle: \"hw5\"\nauthor: \"Jie Cai\"\ndate: \"11/07/2017\"\noutput: html_document\n---\n\n```{r import}\nlibrary(arm)\nlibrary(pROC)\n```\n\n### 1. Chapter 20, Problem 9\n#### It was estimated that the log odds of survival were 3.2-(0.078xage) for females and 1.6-(0.078xage) for males in the Donner Party.\n##### a) What are the estimated probabilities of survival for men and women of ages 25 and 50?\np(Women, Age25) = e^(3.2-0.078x25)/e^(3.2-0.078x25)+1 = 0.777\n\np(Man, Age25) = e^(1.6-0.078x25)/e^(1.6-0.078x25)+1 = 0.413\n\np(Woman, Age50) = e^(3.2-0.078x25)/e^(3.2-0.078x25)+1 = 0.332\n\np(Man, Age50) = e^(1.6-0.078x50)/e^(1.6-0.078x50)+1 = 0.091\n\n##### b) What is the age at which the estimated probability of survival is 50% for women, and for men?\np(Woman, Age?) = e^(3.2-0.078x?)/e^(3.2-0.078x?)+1 = 0.5\n\nAge? = 41.026\n\np(Man, Age?) = e^(1.6-0.078x?)/e^(1.6-0.078x?)+1 = 0.5\n\nAge? = 20.513\n\n### 2. Chapter 20, Problem 16\n```{r Ex2016}\n#read in data\ndf2016 = read.csv('Ex2016.csv')\nattach(df2016)\n\n#make dummy variables for status & age\nn = nrow(df2016)\n\nsurvived = rep(0, n)\nsurvived[Status == \"Survived\"] = 1\n\nperished = rep(0, n)\nperished[Status == \"Perished\"] = 1\n\nadult = rep(0, n)\nadult[AG == \"adult\"] = 1\n\njuvenile = rep(0, n)\njuvenile[AG == \"juvenile\"] = 1\n\n#exploratory data analysis\ntapply(survived, adult, mean)\nbinnedplot(TL, survived, xlab = \"Total Length (mm)\", ylab = \"Status\") \nbinnedplot(AE, survived, xlab = \"Alar Extent Length (mm)\", ylab = \"Status\") \nbinnedplot(WT, survived, xlab = \"Weight (g)\", ylab = \"Status\") \nbinnedplot(BH, survived, xlab = \"Head Length (mm)\", ylab = \"Status\") \nbinnedplot(HL, survived, xlab = \"Humerus Length (inches)\", ylab = \"Status\") \nbinnedplot(FL, survived, xlab = \"Femur Length (inches)\", ylab = \"Status\") \nbinnedplot(TT, survived, xlab = \"Tibio-tarsus Length (inches)\", ylab = \"Status\") \nbinnedplot(SK, survived, xlab = \"Skull Breath (inches)\", ylab = \"Status\") \nbinnedplot(KL, survived, xlab = \"Sternum Length (inches)\", ylab = \"Status\") \n\n#test for multicollinearity to get rid of unneccessary variables\n#threshold: 0.8\ncontVars = df2016[,4:12]\ncor(contVars)\n\n#multicollinearity between: FL & HL, TT & FL\n#since FL correlates strongly with 2 variables, we remove FL.\ndrops <- c(\"FL\")\ndf2016woFL = df2016[ , !(names(df2016) %in% drops)]\n\n#mean-center continous variables to interpret average physical traits\nTL.c = TL - mean(TL)\nAE.c = AE - mean(AE)\nWT.c = WT - mean(WT)\nBH.c = BH - mean(BH)\nHL.c = HL - mean(HL)\nTT.c = TT - mean(TT)\nSK.c = SK - mean(SK)\nKL.c = KL - mean(KL)\n\n#fit untransformed logistic regression\ndf2016.reg1 = glm(survived ~ adult + TL.c + AE.c + WT.c + BH.c + HL.c + TT.c + SK.c + KL.c, family = binomial)\nsummary(df2016.reg1)\nresid.df2016.reg1 = survived - fitted(df2016.reg1)\n\n#we should take out some variables with very high p-values (p>0.5).\n#remove variables: AE, TT\n#create new logistic regression with more removed variables\ndf2016.reg2 = glm(survived ~ adult + TL.c + WT.c + BH.c + HL.c + SK.c + KL.c, family = binomial)\nsummary(df2016.reg2)\nresid.df2016.reg2 = survived - fitted(df2016.reg2)\n\n#no need to check interaction variables\n\n#binned residual plots, just to check if regression is good\ntapply(resid.df2016.reg2, adult, mean)\nbinnedplot(TL.c, resid.df2016.reg2, xlab = \"Total Length (mm)\", ylab = \"Status Residuals\") \nbinnedplot(AE.c, resid.df2016.reg2, xlab = \"Alar Extent Length (mm)\", ylab = \"Status Residuals\") \nbinnedplot(WT.c, resid.df2016.reg2, xlab = \"Weight (g)\", ylab = \"Status Residuals\") \nbinnedplot(BH.c, resid.df2016.reg2, xlab = \"Head Length (mm)\", ylab = \"Status Residuals\") \nbinnedplot(HL.c, resid.df2016.reg2, xlab = \"Humerus Length (inches)\", ylab = \"Status Residuals\") \nbinnedplot(TT.c, resid.df2016.reg2, xlab = \"Tibio-tarsus Length (inches)\", ylab = \"Status Residuals\") \nbinnedplot(SK.c, resid.df2016.reg2, xlab = \"Skull Breath (inches)\", ylab = \"Status Residuals\") \nbinnedplot(KL.c, resid.df2016.reg2, xlab = \"Sternum Length (inches)\", ylab = \"Status Residuals\") \n#all of the residuals look nice and random, so we don't need to transform\n\n#ROC curve\nroc(survived, fitted(df2016.reg2), plot=T, legacy.axes=T)\n#Area under the curve: 0.8992\n#the ROC curve has a large area under the curve (>>0.5)\n#so our logistic regression seems to be a good predictive regression\n\n#confusion matrix\nthreshold = 0.5\ntable(survived, df2016.reg1$fitted > threshold)\n#using a standard threshold of 0.5, the confusion matrix looks pretty good. \n#we have a lot of true positives and negatives.\n\n#co-efficient interpretations\nsummary(df2016.reg2)\nexp(confint.default(df2016.reg2))\n```\nWe are 95% confident that the average odds of survival of an adult house sparrow with average physical features is between 6.528507e-01 and 6.081488e+00. \n\nWe are 95% confident that the average odds of survival of a juvinile house sparrow with average physical features is between 6.528507e-01 + 2.719777e-01 and 6.081488e+00 + 3.679216e+00. \n\nWe are 95% confident that with every mm increase in total length, holding all else constant, we expect the average odds of survival of an adult house sparrow to increase between a factor of 3.644456e-01 and 7.149336e-01.\n\nWe are 95% confident that with every gram increase in weight, holding all else constant, we expect the odds of survival of an adult house sparrow to increase between a factor of 2.334077e-01 and 8.232616e-01.\n\nWe are 95% confident that with every mm increase in head length, holding all else constant, we expect the odds of survival of an adult house sparrow to increase between a factor of 5.849039e-01 and 5.180975e+00.\n\nWe are 95% confident that with every inch increase in humerus length, holding all else constant, we expect the odds of survival of an adult house sparrow to increase between a factor of 1.881523e+08 and 7.256763e+43.\n\nWe are 95% confident that with every inch increase in skull breath, holding all else constant, we expect the odds of survival of an adult house sparrow to increase between a factor of 3.225292e-11 and 4.959903e+34.\n\nWe are 95% confident that with every inch increase in sternum length, holding all else constant, we expect the odds of survival of an adult house sparrow to increase between a factor of 1.905852e+00 and 1.110580e+20.\n\nIn the final logistic regression that I chose, I did not tranform my variables but mean-centered them, and removed variables FF, AE, TT. I found that there was no need to transform my variables since they generated nice exploratory binned plots with no obvious trends (such as hooks, smiley/frowns etc). I mean-centered my variables so I can interpret the intercept as an average of the other physical traits. I also made dummy variables for the discrete variables (survived/perished, adult/juvenile), treating survived/perished as a binary outcome variable and adult as a categorical variable baseline. As for removing variables, I first removed FF because it had obvious multicollinearity effects on other variables, after looking at a correlation matrix. I further removed AE and TT because of their extremely high P-values in the logistic regression, and keeping them would lead to implicit multicollinearity effects on other variables. The random binned residual plots and high area for the ROC curve tells us that we have a good logistic model.\n\nOur logistic regression follows Bumpus's assumptions that the avearge odds of survival of house sparrows increased with an increase in some physical traits (HL, SK and KL), while the odds of survival decreased with an increase in other physical traits (TL, WT and BH). These do not account for other multicollinearity variables (FL) that are closely explained by the traits above, or variables (AE, TT) that are too insignificant in the regression to greatly affect the odds of survival. Following this logic, a house sparrow with superior physical traits leading to high odds of survival would be one with a large Humerus Length, Skull Breath and Sterum Length, and small Total Length, Weight and Head Length.\n\nOne limitation of this model is that we aren't able to predict survival odds of birds with physical features more extreme than our measured dataset, since extraploating can be dangerous. This is especially true for really sentitive variables such as HL, SK, KL. These variables had extremely large confidence interval ranges for their odds of survival since they don't have a large range of measured values in the dataset, and we aren't able to make a very precise prediction for the slopes.\n\n### 3. Chapter 20, Problem 18\n```{r Ex2018}\n#read in data\ndf2018 = read.csv('Ex2018.csv')\nattach(df2018)\n\n#make dummy variables Ford & Tire\nn = nrow(df2018)\n\nford = rep(0, n)\nford[make == \"Ford\"] = 1\n\nother = rep(0, n)\nother[make == \"Other\"] = 1\n\ntire = rep(0, n)\ntire[cause == \"Tire\"] = 1\n  \nnot.tire = rep(0, n)\nnot.tire[cause == \"Not_tire\"] = 1\n\n#exploratory data analysis\ntapply(tire, ford, mean)\nbinnedplot(passengers, tire, xlab = \"# of Passengers\", ylab = \"Tired Related or not Tired Related\") \nbinnedplot(vehicle.age, tire, xlab = \"Vehicle Age (Years)\", ylab = \"Tired Related or not Tired Related\") \n#not a big enough trend/not enough points show evidence that we should use a transformation\n\n#treat vehicle.age as a categorical data\ntapply(tire, vehicle.age, mean)\n#since there is a huge gap between vehicles 0-2 years old and 3+years old, we can split it up into two dummy variables.\n\nnewer = rep(0, n)\nnewer[vehicle.age <=2] = 1\n\nolder = rep(0,n)\nolder[vehicle.age >=3] = 1\n\n#mean-center passengers, to interpret for average values\npassengers.c = passengers - mean(passengers)\n\n#create interaction variable between passengers and tire\npassenger.ford.interact.c = passengers.c*ford\n\n#fit untransformed logistic regression with interaction variable\ndf2018.reg1 = glm(tire ~ ford + older + passengers.c + passenger.ford.interact.c, family = binomial)\nsummary(df2018.reg1)\nresid.df2018.reg1 = tire -fitted(df2018.reg1)\n\n#fit untransformed logistic regression without interaction variable\ndf2018.reg2 = glm(tire ~ ford + older + passengers.c, family = binomial)\nsummary(df2018.reg2)\nresid.df2018.reg2 = tire -fitted(df2018.reg2)\n\n#ANOVA test to see if interaction variable is significant\nanova(df2018.reg1, df2018.reg2, test= \"Chisq\")\n#p = 0.01708\n#we get a very small p-value for our ANOVA test, so the interaction variable is important\n\n#check residual plots\ntapply(resid.df2018.reg1, ford, mean)\ntapply(resid.df2018.reg1, older, mean)\nbinnedplot(passengers.c, resid.df2018.reg1, xlab = \"Mean Centered Passengers\", ylab = \"Tire Residuals\") \nbinnedplot(passenger.ford.interact.c, resid.df2018.reg1, xlab = \"Mean Centered Passengers * Tire\", ylab = \"Tire Residuals\") \n#all of the residuals look nice and random, so we don't need to transform\n\n#ROC curve\nroc(tire, fitted(df2018.reg1), plot=T, legacy.axes=T)\n#Area under the curve: 0.9336\n#the ROC curve has a large area under the curve (>>0.5)\n#so our logistic regression seems to be a good predictive regression\n\n#confusion matrix\nthreshold = 0.5\ntable(tire, df2018.reg1$fitted > threshold)\n#using a standard threshold of 0.5, the confusion matrix looks pretty good. \n#we have a lot of true positives.\n\n#co-efficient interpretations\nsummary(df2018.reg1)\nexp(confint.default(df2018.reg1))\n```\n\nWe are 95% confident that the average odds that a fatal car incident (of a an average older Ford car) due to tire causes is between 3.672773e-05 and 1.124668e-03. \n\nWe are 95% confident that the average odds that a fatal car incident (of an average older non-Ford car) due to tire causes is between 3.672773e-05 + 2.811339e+00 + 1.033319e+00 and 1.124668e-03 + 2.560136e+01 + 3.856288e+00.  \n\nWe are 95% confident that the average odds that a fatal car incident (of a average newer Ford car) due to tire causes is between 3.672773e-05 + 2.811339e+00 and 1.124668e-03 + 2.560136e+01. \n\nWe are 95% confident that with every one increase with passenger count, holding all else constant, we expect the odds that a fatal car incident (of a an average older Ford car) to increase between 6.856114e-01 + 1.033319e+00 and 2.235469e+00 + 3.856288e+00.\n\nIn the final logistic regression that I chose, I did not tranform my variables but mean-centered passenger count, and included a continous interaction variable between car brand and number of passengers (ford*passengers.c). I found that there was no need to transform my variables since they generated nice exploratory binned plots with no obvious trends (such as hooks, smiley/frowns etc). I mean-centered passengers so I can interpret in terms of average amount of passengers. I also made dummy variables for ford, tire cause and car age (ford/non-ford, tire/non-tire, newer/older), treating tire/non-tire as a binary outcome variable, while treating ford and older as a categorical variable baselines. I changed car age into a discrete variable since there was a really big gap between cars of ages 0-2, and cars ages 3+, which split the continous variable into perfect discete halves (0/1). As for interaction variables, the textbook question suggested we look at interaction effects between ford and passengers. After doing an ANOVA test and finding a small P-value, we confirmed that this interaction variable is significant. The random binned residual plots and high area for the ROC curve tells us that we have a good logistic model.\n\nThe negative slope in tire-related odds between (older) Ford cars and (older) non-Ford cars suggets that there does seem to be some big difference between their odds, espesically considering the vairable's small p-value. However, since car-brand (Ford/non-Ford) is highly intereactive with passengers, the amount of passengers also has some effect on whether a car accident was tire related or not.\n\nOne limitation of this model is that we can't really conclude that an increase in tire related incident odds are due to Ford cars or passenger counts. We can only analyze that these variables have a correlation, but we cannot conclude that Ford-brand cars and passenger counts cause tire related accidents. Another limitation is that we cannot extrapolate, so we cannot accurately predict tire-related odds/probabilities for cars that are 6+ years old, nor can we predict for passenger counts that are greater than 11.",
    "created" : 1513199806628.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4251008379",
    "id" : "4D56E721",
    "lastKnownWriteTime" : 1510244010,
    "last_content_update" : 1510244010,
    "path" : "~/Documents/DUKE/Sophomore/Semester1/STA210/hw/hw5/hw5.Rmd",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 6,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}